<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">
    <link rel="shortcut icon" href="../../assets/ico/favicon.ico">

    <title>Jingjing Wang</title>

    <!-- Bootstrap core CSS -->
    <link href="./dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Just for debugging purposes. Don't actually copy this line! -->
    <!--[if lt IE 9]><script src="../../assets/js/ie8-responsive-file-warning.js"></script><![endif]-->

    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- Custom styles for this template -->
    <link href="jumbotron.css" rel="stylesheet">
    <link href="dist/css/particles.css" rel="stylesheet">
</head>

<body>

    <div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
        <div class="container">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <!--
          <a class="navbar-brand" href="#">Project name</a>
          -->
            </div>
            <div class="navbar-collapse collapse">
                <ul class="nav nav-pills pull-right">
                    <li class="active"><a href="#">Home</a></li>
                    <li><a href="#Recent Research">Research</a></li>
                    <li><a href="#publication">Publications</a></li>
                    <!-- <li><a href="#award">Awards</a></li> -->
                    <!-- <li><a href="data.html">Data</a></li> -->
                </ul>
            </div><!--/.navbar-collapse -->
        </div>
    </div>

    <!-- Main jumbotron for a primary marketing message or call to action -->
    <div class="jumbotron">
        <div id="particles"></div>
        <div class="container">
            <div class="row">
                <div class="col-md-2" style="margin-right: 0px;">
                    <img height="160" src="img/djwang.png" align="left" hspace="6"
                        style="margin-left:-6p;margin-right:10px">
                </div>
                <div class="col-md-9">
                    <b>
                        <font size="5"> Jingjing Wang 王晶晶 </font>
                    </b>
                    <p></p>
                    <p> <i>Associate Professor, School of Computer Science, Soochow University </i></p>
                    <p> <i> <b> Email:</b> djingwang [at] suda [dot] edu [dot] cn</i> </p>
                    <p> <i><b> Address:</b> Shizi Street 1#, Suzhou, China 215006 </i>
                    </p>
                </div>
            </div>

            <div class="row" id="About">
                <div class="col-md-12">
                    <p> I am an Associate Professor of <a target="_blank" href="https://scst.suda.edu.cn/">Computer
                            Science</a> at <a target="_blank" href="http://www.suda.edu.cn/">Soochow University</a>.
                        I am also a Senior Technical Consultant (Part-time) at Microsoft (Asia), China.
                        My research interests focus on Multimodal Computing (especially for Visual-Language
                        Understanding and Generation), Natural Language Processing, Large Language Models, and AI for
                        Medical.
                        I received my Ph.D. degree from <a target="_blank" href="http://www.suda.edu.cn/">Soochow
                            University</a> in 2019.
                    </p>
                </div>
            </div>

            <div class="row" id="Research">
                <div class="col-md-12">
                    <h2>Research Interests</h2>
                    <p>My research interests span a wide range of topics, including, but not limited to, Multimodal
                        Computing (especially for Visual-Language Understanding and Generation), Natural Language
                        Processing, Large Language Models, and AI for Medical.</p>
                </div>
            </div>

            <div class="row" id="Recent Research">
                <div class="col-md-12">

                    <div class="page-header">
                        <h2>Recent Research</h2>
                    </div>

                    <div class="page-header">
                        <h4>Foundation Model for Video Anomaly Detection</h4>
                    </div>
                    <img width="600" src="img/hawkeye.svg" align="left" style="margin-left:0px;margin-right:20px">
                    <p>
                        The goal is to establish a unified framework for video anomaly detection, advancing precision in
                        identifying and localizing abnormal events across dynamic scenes while enabling interpretable
                        analysis
                        of complex visual patterns.
                    </P>
                    <p>
                        Starting from real-world applications in surveillance and social media analysis, we introduce
                        Hawkeye (<a target="_blank" href="works/Hawkeye Discovering and Grounding Implicit Anoma-lous Sentiment in Recon-videos via Scene-enhanced Video Large Language Model.pdf">Zhang et al., ACM MM'24</a>)
                        , the first scene-enhanced video-language model designed for anomaly detection. Hawkeye
                        integrates multimodal context
                        (visual-textual-temporal cues) to recognize subtle anomalies and pinpoint their temporal
                        boundaries in untrimmed videos,
                        This work lays a critical foundation for event typing and spatiotemporal
                        localization in short video understanding.
                    </p>
                    <p>
                        Building on this, we investigate low-resource scenarios where annotated anomaly data is scarce.
                        Our Continuous
                        Attention Modeling method (<a target="_blank" href="works/针对低资源场景下连续情感分析任务的持续注意力建模.pdf">Zhang et al.,JOS'24</a>) enhances adaptability by capturing long-range
                        dependencies
                        in sparse anomaly signals. Further, we extend Hawkeye with self-supervised learning to uncover
                        latent patterns
                        across unlabeled videos, improving generalization to unseen anomaly types.
                        To scale solutions, we construct a benchmark suite combining large-scale anomaly annotations and
                        instruction-tuned datasets.
                        This addresses the challenge of diverse event types (e.g., accidents, unusual behaviors) and
                        supports downstream tasks
                        like explainable reasoning (<a target="_blank" href="works/ChatASU Evoking LLM’s Reflexion to Truly Understand Aspect Sentiment in Dialogues.pdf">Liu et al., COLING'2024</a>).
                    </p>
                    <!-- <p>
                        目标​​是构建统一的视频异常检测框架，在动态场景中提升异常事件识别与定位的精度，同时实现对复杂视觉模式的可解释分析。
                        ​从安防监控与社交媒体分析的实际需求出发​​，我们提出首个场景增强的视频-语言模型Hawkeye（ACM MM-2024）。该模型融合视觉、
                        文本与时序线索，能够识别短视频中细微的异常行为并精确定位其时空边界（如图2.1所示），为短时视频的事件类型划分与时空定位奠定重要基础。
                        ​​针对低资源场景​​（标注数据稀缺），我们提出连续注意力建模方法（软件学报，张等人），通过捕捉稀疏异常信号的长程依
                        赖关系提升模型适应性。进一步扩展Hawkeye至自监督学习框架，挖掘未标注视频的潜在模式以增强对未见异常类型的泛化能力
                        （赵等人，审稿中）。                
                        ​​为扩大解决方案规模​​，我们构建了融合大规模异常标注与指令微调数据集的基准测试套件，解决事件类型多样性（如事故、异常行为）
                        的挑战，并支持可解释推理等下游任务（刘等人，COLING-2024）。
                    </p> -->
                    <p>
                        <b>Papers: </b>
                        (<a target="_blank" href="works/Hawkeye Discovering and Grounding Implicit Anoma-lous Sentiment in Recon-videos via Scene-enhanced Video Large Language Model.pdf">Zhang et al., ACM MM'24</a>)
                        (<a target="_blank" href="works/针对低资源场景下连续情感分析任务的持续注意力建模.pdf">Zhang et al.,JOS'24</a>)
                        (<a target="_blank" href="works/ChatASU Evoking LLM’s Reflexion to Truly Understand Aspect Sentiment in Dialogues.pdf">Liu et al., COLING'2024</a>).
                    </p>

                    <div class="page-header">
                        <!-- <h4>Foundation Model for Graphs</h4> -->
                    </div>
                    <img width="600" src="img/L3EM.svg" align="left"
                        style="margin-left:0px;margin-right:20px;margin-bottom:10px">
                    <p>
                        The goal​​ is to establish an emotion-driven framework for multimodal large language model (LLM) video generation
                        , achieving cross-modal semantic alignment and high-fidelity emotional action synthesis while enhancing 
                        controllability and realism in generated content.
                    </p>
                    <p>
                        Starting from the core demand of multimodal interaction, we propose LLM-Guided Emotion-Action Synthesis 
                        (<a target="_blank" href="works/Towards Emotion-enriched Text-to-Motion Generation via LLM-guided Limb-level Emotion Manipulating.pdf">Yu et al., ACM MM'2024</a>),
                         the first method to jointly model text emotion semantics and human motion sequences for emotionally rich video generation 
                         This work enables precise conveyance of complex emotions (e.g., "excited waving") in synthetic 
                         videos, providing critical support for emotion-aware short video editing.
                    </p>
                    <p>
                        To address cross-modal alignment challenges, we introduce two innovations: 
                        1) A Trustworthy Reflection Mechanism 
                        (<a target="_blank" href="works/ChatASU Evoking LLM’s Reflexion to Truly Understand Aspect Sentiment in Dialogues.pdf">Liu et al., COLING'24</a>) 
                        that dynamically calibrates generation intent with emotional 
                        labels, improving semantic coherence in action outputs; 
                        2) Latent Diffusion Models 
                        (<a target="_blank" href="works/How to Understand 'Support'? An Implicit-enhanced Causal Inference Approach for Weakly-supervised Phrase Grounding.pdf">Luo et al., COLING'24</a>) 
                        that jointly model emotion-themed distributions across text, audio, 
                        and motion modalities in latent space, overcoming efficiency bottlenecks of pixel-level generation.

                        To scale practical applications, we construct the first benchmark dataset for emotion-driven action generation, 
                        containing 100K+ emotion-annotated human motion sequences and multimodal instruction pairs. Based on this, we propose 
                        AutoEmoDirector, a framework enabling users to edit video emotion intensity and motion styles via natural language instructions 
                        (e.g., "transform a joyful dance into a sorrowful stroll"), significantly lowering the barrier for video content creation.
                    </p>
                    
                    <!-- <p>
                        目标​​是构建情感驱动的大模型视频生成框架，实现跨模态语义对齐与高保真情感动作合成，同时提升生成结果的可控性与真实性。
                        ​​从多模态交互的核心需求出发​​，我们提出LLM-Guided Emotion-Action Synthesis（ACM MM-2024），通过大语言模型引导肢体
                        动作的细粒度情感表达（如图2.2所示）。该方法首次将文本情感语义与人体运动序列联合建模，生成的视频能精准传递愤怒、喜
                        悦等复杂情感（如“激动地挥手”），为情感化短视频编辑提供关键技术支持。  
                        ​​针对跨模态对齐难题​​，我们提出两种创新方法：1）改进大语言模型的可信反思机制（Liu et al., COLING-2024），通过动态校
                        准生成意图与情感标签的一致性，提升动作生成的语义合理性；2）设计隐空间扩散模型（Luo et al., COLING-2024），在潜在
                        空间联合建模文本、音频与动作模态的情感主题分布，突破传统像素级生成的效率瓶颈。
                        ​​为推动规模化应用​​，我们构建了首个情感动作生成基准数据集，包含10万+条带情感标注的人体动作序列与多模态指令对。基于此，
                        我们提出AutoEmoDirector框架，支持用户通过自然语言指令实时编辑视频中的情感强度与动作风格（如“将欢快舞蹈转为悲伤漫步
                        ”），显著降低视频创作门槛。
                    </p> -->
                    
                    <p>
                    <b>Papers: </b>
                    (<a target="_blank" href="works/Towards Emotion-enriched Text-to-Motion Generation via LLM-guided Limb-level Emotion Manipulating.pdf">Yu et al., ACM MM'2024</a>)
                    (<a target="_blank" href="works/How to Understand 'Support'? An Implicit-enhanced Causal Inference Approach for Weakly-supervised Phrase Grounding.pdf">Luo et al., COLING'24</a>)
                    (<a target="_blank" href="works/ChatASU Evoking LLM’s Reflexion to Truly Understand Aspect Sentiment in Dialogues.pdf">Liu et al., COLING'24</a>)
                    </p>
                </div>


            </div>

            <div class="row">
                <div class="col-md-12">
                    <h3>Academic Services</h3>
                    <ul>
                        <li>
                            <p><b>Technical Program Committee (Area Chair & PC)</b></p>
                        </li>
                        <li>
                            <p>ACL: Annual Meeting of the Association for Computational Linguistics, Area Chair</p>
                        </li>
                        <li>
                            <p>EMNLP: Conference on Empirical Methods in Natural Language Processing, Area Chair</p>
                        </li>
                        <li>
                            <p>AAAI: Association for the Advancement of Artificial Intelligence, PC</p>
                        </li>
                        <li>
                            <p>IJCAI: International Joint Conference on Artificial Intelligence, PC</p>
                        </li>

                        <li>
                            <p><b>Journal Reviewer</b></p>
                        </li>
                        <li>
                            <p>TASLP: IEEE/ACM Transactions on Audio, Speech, and Language Processing</p>
                        </li>
                        <li>
                            <p>TALLIP: ACM Transactions on Asian and Low-Resource Language Information Processing</p>
                        </li>
                        <li>
                            <p>SCIS: Science China Information Sciences</p>
                        </li>
                        <li>
                            <p>Science China</p>
                        </li>
                        <li>
                            <p>Acta Automatica Sinica</p>
                        </li>
                        <li>
                            <p>Journal of Chinese Information Processing</p>
                        </li>
                    </ul>
                </div>
            </div>


            <div class="row" id="publication">
                <div class="col-md-12">
                    <h2>Selected Publications</h2>
                    <div class="sectionbody">
                        <div class="paragraph">
                            <p>2025</p>
                        </div>
                        <div class="olist arabic">
                            <ol class="arabic">
                                <li>
                                    <p>
                                        Ma, J., <strong>Wang, J.</strong>, Luo, J., Yu, P., & Zhou, G. <em>Sherlock:
                                            Towards Multi-scene Video Abnormal Event Extraction and Localization via a
                                            Global-local Spatial-sensitive LLM</em>. The Web Conference
                                        (<strong>WWW</strong>), 2025. (CCF A)
                                    </p>
                                </li>
                                <li>
                                    <p>
                                        Luo, J., <strong>Wang, J.</strong>, Ma, J., Jin, Y., Li, S., & Zhou, G.
                                        <em>Omni-SILA: Towards Omni-scene Driven Visual Sentiment Identifying, Locating
                                            and Attributing in Videos</em>. The Web Conference (<strong>WWW</strong>),
                                        2025. (CCF A)
                                    </p>
                                </li>
                                <li>
                                    <p>
                                        Cen, J., Liu, J., Li, Z., & <strong>Wang, J.</strong> <em>SQLFixAgent: Towards
                                            Semantic-Accurate Text-to-SQL Parsing via Consistency-Enhanced Multi-Agent
                                            Collaboration</em>. The AAAI Conference on Artificial Intelligence
                                        (<strong>AAAI</strong>), 2025. (CCF A)
                                    </p>
                                </li>
                            </ol>
                        </div>

                        <div class="paragraph">
                            <p>2024</p>
                        </div>
                        <div class="olist arabic">
                            <ol class="arabic">
                                <li>
                                    <p>
                                        Zhao J, <strong>Wang, J.</strong>, Jin Y, et al. <em>Hawkeye: Discovering and
                                            Grounding Implicit Anomalous Sentiment in Recon-videos via Scene-enhanced
                                            Video Large Language Model</em>. Proceedings of ACM International Conference
                                        on Multimedia (<strong>MM</strong>), 2024, 592–601. (CCF A)
                                    </p>
                                </li>
                                <li>
                                    <p>
                                        Yu T, <strong>Wang, J.</strong>, Wang J, et al. <em>Towards Emotion-enriched
                                            Text-to-Motion Generation via LLM-guided Limb-level Emotion
                                            Manipulating</em>. Proceedings of ACM International Conference on Multimedia
                                        (<strong>MM</strong>), 2024, 612–621. (CCF A)
                                    </p>
                                </li>
                                <li>
                                    <p>
                                        Liu Y, <strong>Wang, J.</strong>, Luo J, et al. <em>ChatASU: Evoking LLM's
                                            Reflexion to Truly Understand Aspect Sentiment in Dialogues</em>.
                                        Proceedings of International Conference on Computational Linguistics
                                        (<strong>COLING</strong>), 2024, 3075–3085. (CCF B)
                                    </p>
                                </li>
                                <li>
                                    <p>
                                        Luo J, Zhao J, <strong>Wang, J.</strong>, et al. <em>How to Understand
                                            'Support'? An Implicit-enhanced Causal Inference Approach for
                                            Weakly-supervised Phrase Grounding</em>. International Conference on
                                        Computational Linguistics (<strong>COLING</strong>), 2024. (CCF B)
                                    </p>
                                </li>
                                <li>
                                    <p>
                                        Luo, J., <strong>Wang, J.</strong>, & Zhou, G. <em>TopicDiff: A Topic-enriched
                                            Diffusion Approach for Multimodal Conversational Emotion Detection</em>.
                                        International Conference on Computational Linguistics (<strong>COLING</strong>),
                                        2024. (CCF B)
                                    </p>
                                </li>
                                <li>
                                    <p>
                                        Yu, T., <strong>Wang, J.</strong>, Luo, J., Wang, J., & Zhou, G. <em>TACL: A
                                            Trusted Action-enhanced Curriculum Learning Approach to Multimodal Affective
                                            Computing</em>. Neurocomputing, 2024. (SCI Q2)
                                    </p>
                                </li>
                            </ol>
                        </div>

                        <div class="paragraph">
                            <p>2023</p>
                        </div>
                        <div class="olist arabic">
                            <ol class="arabic">
                                <li>
                                    <p>
                                        Luo JM, <strong>Wang JJ</strong>, Zhou GD. <em>Multi-modal Reliability-aware
                                            Affective Computing</em>. Ruan Jian Xue Bao/Journal of Software, 2023,
                                        36(2):537-553. (Chinese Core Journal)
                                    </p>
                                </li>
                                <li>
                                    <p>
                                        Liu YD, <strong>Wang JJ</strong>, Luo JM, Zhou GD. <em>LLM-Grounded Conversation
                                            Aspect Sentiment Understanding via Muti-Agent Consistency Reflection</em>.
                                        Journal of Software, 2023. (Chinese Core Journal)
                                    </p>
                                </li>
                                <li>
                                    <p>
                                        Zhang H, <strong>Wang JJ</strong>, Luo JM, Zhou GD. <em>Continual Attention
                                            Modeling for Sucessive Sentiment Analysis in Low resource Scenarios</em>.
                                        Ruan Jian Xue Bao/Journal of Software, 2023, 35(12):5470-5486. (Chinese Core
                                        Journal)
                                    </p>
                                </li>
                            </ol>
                        </div>

                        <div class="paragraph">
                            <p>2022</p>
                        </div>
                        <div class="olist arabic">
                            <ol class="arabic">
                                <li>
                                    <p>
                                        Gao X, <strong>Wang, J.</strong>, Li S, et al. <em>Cognition-driven multimodal
                                            personality classification</em>. Science China Information Sciences
                                        (<strong>SCIS</strong>), 2022, 65(10). (CCF A)
                                    </p>
                                </li>
                                <li>
                                    <p>
                                        Gao, X., <strong>Wang, J.</strong>, Li, S., & Zhou, G. <em>Cognition-Driven
                                            Real-Time Personality Detection via Language-Guided Contrastive Visual
                                            Attention</em>. IEEE International Conference on Multimedia and Expo
                                        (<strong>ICME</strong>), 2021, 1-6. (CCF B)
                                    </p>
                                </li>
                            </ol>
                        </div>

                        <div class="paragraph">
                            <p>2021</p>
                        </div>
                        <div class="olist arabic">
                            <ol class="arabic">
                                <li>
                                    <p>
                                        <strong>Wang, J.</strong>, Luo, J., & Zhou, G. <em>Fine-Grained Question-Answer
                                            Matching via Sentence-Aware Contrastive Self-supervised Transfer</em>. CCF
                                        International Conference on Natural Language Processing and Chinese Computing,
                                        2021, 616-628. (CCF C)
                                    </p>
                                </li>
                            </ol>
                        </div>

                        <div class="paragraph">
                            <p>2020</p>
                        </div>
                        <div class="olist arabic">
                            <ol class="arabic">
                                <li>
                                    <p>
                                        Chen X, Sun C, <strong>Wang, J.</strong>, et al. <em>Aspect Sentiment
                                            Classification with Document-level Sentiment Preference Modeling</em>.
                                        Proceedings of Annual Meeting of the Association for Computational Linguistics
                                        (<strong>ACL</strong>), 2020, 3667–3677. (CCF A)
                                    </p>
                                </li>
                                <li>
                                    <p>
                                        <strong>Wang, J.</strong>, Wang, J., Sun, C., Li, S., Liu, X., Si, L., ... &
                                        Zhou, G. <em>Sentiment classification in customer service dialogue with
                                            topic-aware multi-task learning</em>. Proceedings of the AAAI Conference on
                                        Artificial Intelligence (<strong>AAAI</strong>), 2020, 9177-9184. (CCF A)
                                    </p>
                                </li>
                                <li>
                                    <p>
                                        An, M., <strong>Wang, J.</strong>, Li, S., & Zhou, G. <em>Multimodal
                                            topic-enriched auxiliary learning for depression detection</em>. Proceedings
                                        of the International Conference on Computational Linguistics
                                        (<strong>COLING</strong>), 2020, 1078-1089. (CCF B)
                                    </p>
                                </li>
                            </ol>
                        </div>
                    </div>
                </div>
            </div>

            <div class="row" id="publication-before-2019">
                <div class="col-md-12">
                    <h2>Selected Publications Before 2019</h2>
                    <div class="sectionbody">
                        <div class="paragraph">
                            <p>2019</p>
                        </div>
                        <div class="olist arabic">
                            <ol class="arabic">
                                <li>
                                    <p>
                                        <strong>Jingjing Wang</strong>, Changlong Sun, Shoushan Li, Jiancheng Wang, Luo
                                        Si, Min Zhang, Xiaozhong Liu, Guodong Zhou. <em>Human-Like Decision Making:
                                            Document-level Aspect Sentiment Classification via Hierarchical
                                            Reinforcement Learning</em>. Proceedings of the Conference on Empirical
                                        Methods in Natural Language Processing (<strong>EMNLP</strong>), 2019,
                                        5585-5594. (CCF B)
                                    </p>
                                </li>
                                <li>
                                    <p>
                                        <strong>Jingjing Wang</strong>, Changlong Sun, Shoushan Li, Xiaozhong Liu, Min
                                        Zhang, Luo Si, Guodong Zhou. <em>Aspect Sentiment Classification Towards
                                            Question-Answering with Reinforced Bidirectional Attention Network</em>.
                                        Proceedings of Annual Meeting of the Association for Computational Linguistics
                                        (<strong>ACL</strong>), 2019. (CCF A)
                                    </p>
                                </li>
                            </ol>
                        </div>

                        <div class="paragraph">
                            <p>2018</p>
                        </div>
                        <div class="olist arabic">
                            <ol class="arabic">
                                <li>
                                    <p>
                                        <strong>Jingjing Wang</strong>, Jie Li, Shoushan Li, Yangyang Kang, Min Zhang,
                                        Luo Si, Guodong Zhou. <em>Aspect Sentiment Classification with both Word-level
                                            and Clause-level Attention Networks</em>. Proceedings of International Joint
                                        Conference on Artificial Intelligence (<strong>IJCAI</strong>), 2018, 4439-4445.
                                        (CCF A)
                                    </p>
                                </li>
                                <li>
                                    <p>
                                        <strong>Jingjing Wang</strong>, Shoushan Li, Mingqi Jiang, Hanqian Wu, Guodong
                                        Zhou. <em>Cross-media User Profiling with Joint Textual and Social User
                                            Embedding</em>. Proceedings of International Conference on Computational
                                        Linguistics (<strong>COLING</strong>), 2018, 246-251. (CCF B)
                                    </p>
                                </li>
                                <li>
                                    <p>
                                        Chenlin Shen, Changlong Sun, <strong>Jingjing Wang</strong>, Yangyang Kang,
                                        Shoushan Li, Xiaozhong Liu, Luo Si, Min Zhang, Guodong Zhou. <em>Sentiment
                                            Classification towards Question-Answering with Hierarchical Matching
                                            Network</em>. Proceedings of Conference on Empirical Methods in Natural
                                        Language Processing (<strong>EMNLP</strong>), 2018, 3654-3663. (CCF B)
                                    </p>
                                </li>
                            </ol>
                        </div>

                        <div class="paragraph">
                            <p>2017</p>
                        </div>
                        <div class="olist arabic">
                            <ol class="arabic">
                                <li>
                                    <p>
                                        <strong>Jingjing Wang</strong>, Shoushan Li, Guodong Zhou. <em>Joint Learning on
                                            Relevant User Attributes in Micro-blog</em>. Proceedings of International
                                        Joint Conference on Artificial Intelligence (<strong>IJCAI</strong>), 2017,
                                        4130-4136. (CCF A)
                                    </p>
                                </li>
                            </ol>
                        </div>

                        <div class="paragraph">
                            <p>2015</p>
                        </div>
                        <div class="olist arabic">
                            <ol class="arabic">
                                <li>
                                    <p>
                                        <strong>Jingjing Wang</strong>, Yunxia Xue, Shoushan Li, Guodong Zhou.
                                        <em>Leveraging Interactive Knowledge and Unlabeled Data in Gender Classification
                                            with Co-training</em>. Proceedings of International Conference on Database
                                        Systems for Advanced Applications (<strong>DASFAA</strong>), 2015, 246-251. (CCF
                                        B)
                                    </p>
                                </li>
                                <li>
                                    <p>
                                        Shoushan Li, <strong>Jingjing Wang</strong>, Guodong Zhou, Hanxiao Shi.
                                        <em>Interactive Gender Inference with Integer Linear Programming</em>.
                                        Proceedings of International Joint Conference on Artificial Intelligence
                                        (<strong>IJCAI</strong>), 2015, 2341-2347. (CCF A)
                                    </p>
                                </li>
                                <li>
                                    <p>
                                        Shoushan Li, Lei Huang, <strong>Jingjing Wang</strong>, Guodong Zhou.
                                        <em>Semi-Stacking for Semi-supervised Sentiment Classification</em>. Proceedings
                                        of Annual Meeting of the Association for Computational Linguistics
                                        (<strong>ACL</strong>), 2015, 27-31. (CCF A)
                                    </p>
                                </li>
                                <li>
                                    <p>
                                        Zhu zhu, <strong>Jingjing Wang</strong>, Shoushan Li, Guodong Zhou.
                                        <em>Interactive Gender Inference in Social Media</em>. Proceedings of
                                        International Conference on Database Systems for Advanced Applications
                                        (<strong>DASFAA</strong>), 2015, 252-258. (CCF B)
                                    </p>
                                </li>
                            </ol>
                        </div>
                    </div>
                </div>
            </div>

            <div class="row" id="award">
                <div class="col-md-12">
                    <h2>Awards & Honors</h2>
                    <ul>
                        <li>
                            <p>Outstanding Graduate PhD Student of Soochow University (2019)</p>
                        </li>
                        <li>
                            <p>Suzhou Industrial Park Scholarship (2018)</p>
                        </li>
                        <li>
                            <p>National Scholarship for Ph.D. (2017)</p>
                        </li>
                        <li>
                            <p>Ph.D. Scholarship of Soochow University (2017)</p>
                        </li>
                        <li>
                            <p>National Scholarship for Master (2016)</p>
                        </li>
                        <li>
                            <p>Outstanding Graduate Student of Soochow University (2016)</p>
                        </li>
                        <li>
                            <p>Suzhou Industrial Park Scholarship (2015)</p>
                        </li>
                        <li>
                            <p>Master Scholarship of Soochow University (2015)</p>
                        </li>
                    </ul>
                </div>
            </div>

        </div>
    </div>


    <div class="container">
        <!-- Example row of columns -->

        <hr>

        <footer>
            <p>&copy; Jingjing Wang 2023</p>
        </footer>
    </div> <!-- /container -->


    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
    <script src="./dist/js/bootstrap.min.js"></script>
    <script src="js/particles.min.js"></script>
    <script>
        document.addEventListener('DOMContentLoaded', function () {
            particlesJS('particles', {
                "particles": {
                    "number": {
                        "value": 80,
                        "density": {
                            "enable": true,
                            "value_area": 800
                        }
                    },
                    "color": {
                        "value": "#cccccc"
                    },
                    "shape": {
                        "type": "circle",
                        "stroke": {
                            "width": 0,
                            "color": "#000000"
                        }
                    },
                    "opacity": {
                        "value": 0.5,
                        "random": false
                    },
                    "size": {
                        "value": 3,
                        "random": true
                    },
                    "line_linked": {
                        "enable": true,
                        "distance": 150,
                        "color": "#cccccc",
                        "opacity": 0.4,
                        "width": 1
                    },
                    "move": {
                        "enable": true,
                        "speed": 2,
                        "direction": "none",
                        "random": false,
                        "straight": false,
                        "out_mode": "out",
                        "bounce": false
                    }
                },
                "interactivity": {
                    "detect_on": "canvas",
                    "events": {
                        "onhover": {
                            "enable": true,
                            "mode": "grab"
                        },
                        "onclick": {
                            "enable": true,
                            "mode": "push"
                        },
                        "resize": true
                    },
                    "modes": {
                        "grab": {
                            "distance": 140,
                            "line_linked": {
                                "opacity": 1
                            }
                        },
                        "push": {
                            "particles_nb": 4
                        }
                    }
                },
                "retina_detect": true
            });
        });
    </script>
</body>

</html>