<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">
    <link rel="shortcut icon" href="../../assets/ico/favicon.ico">

    <title>Jingjing Wang</title>

    <!-- Bootstrap core CSS -->
    <link href="../dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Just for debugging purposes. Don't actually copy this line! -->
    <!--[if lt IE 9]><script src="../../assets/js/ie8-responsive-file-warning.js"></script><![endif]-->

    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- Custom styles for this template -->
    <link href="jumbotron.css" rel="stylesheet">
    <link href="../dist/css/particles.css" rel="stylesheet">
    <style>
        .model-img {
            position: relative;
            min-height: 300px;
            background-color: #f5f5f5;
        }

        .model-img::before {
            content: "Loading...";
            position: absolute;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            color: #666;
        }
    </style>
</head>

<body>

    <div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
        <div class="container">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <!--
          <a class="navbar-brand" href="#">Project name</a>
          -->
            </div>
            <div class="navbar-collapse collapse">
                <ul class="nav nav-pills pull-right">
                    <li class="active"><a href="#">Home</a></li>
                    <li><a href="#Recent Research">Research</a></li>
                    <li><a href="#publication">Publications</a></li>
                    <!-- <li><a href="#award">Awards</a></li> -->
                    <!-- <li><a href="data.html">Data</a></li> -->
                </ul>
            </div><!--/.navbar-collapse -->
        </div>
    </div>

    <!-- Main jumbotron for a primary marketing message or call to action -->
    <div class="jumbotron">
        <!-- <div id="particles"></div> -->
        <div class="container">
            <div class="row">
                <div class="col-md-2" style="margin-right: 0px;">
                    <img height="160" src="../img/djwang.png" align="left" hspace="6"
                        style="margin-left:-6p;margin-right:10px">
                </div>
                <div class="col-md-9">
                    <b>
                        <font size="5"> Jingjing Wang 王晶晶 </font>
                    </b>
                    <p></p>
                    <p> <i>Associate Professor, School of Computer Science, Soochow University </i></p>
                    <p> <b> Email:</b> <i> djingwang [at] suda [dot] edu [dot] cn</i> </p>
                    <p> <b> Address:</b> <i>Shizi Street 1#, Suzhou, China 215006 </i>
                    </p>
                </div>
            </div>

            <div class="row" id="About">
                <div class="col-md-12">
                    <p> I am an Associate Professor of <a target="_blank" href="https://scst.suda.edu.cn/">Computer
                            Science</a> at <a target="_blank" href="http://www.suda.edu.cn/">Soochow University</a>.
                        I am also a Senior Technical Consultant (Part-time) at Microsoft (Asia), China.
                        My research interests focus on Multimodal Computing (especially for Visual-Language
                        Understanding and Generation), Natural Language Processing, Large Language Models, and AI for
                        Medical.
                        I received my Ph.D. degree from <a target="_blank" href="http://www.suda.edu.cn/">Soochow
                            University</a> in 2019, advised by <a target="_blank"
                            href="https://scholar.google.com/citations?user=KELQj9cAAAAJ&hl=zh-CN">Prof. Guodong Zhou</a>. During my career, I am also working with <a target="_blank"
                            href="https://scholar.google.com/citations?user=CncXH-YAAAAJ&hl=en">Prof. Min Zhang</a>.

                    </p>
                    <p>
                        <font color="red">I am actively seeking dedicated students with <u>intellectual curiosity and a
                                strong work ethic</u> to join my research team.</font>
                        Prospective candidates are welcome to submit their academic CV and research interests via email
                        for initial consultation.
                        Regarding recommendation letters, please be advised that I can only provide substantive
                        evaluations for candidates with whom I have maintained at least six months of meaningful
                        academic collaboration.
                        This duration allows me to objectively assess your research competencies, scholarly
                        contributions, and professional development through sustained engagement.
                    </p>
                </div>
            </div>


            <div class="row" id="What's New">
                <div class="col-md-12">
                    <div class="page-header">
                        <h3><b>What's New</b></h3>

                        <li style="margin:10px"> <b>[Jan. 21, 2025]</b> Two Papers ("Omni-SILA" & "Sherlock") are accepted by WWW 2025. </li>
                        
                        <li style="margin:10px"> <b>[Dec. 28, 2024]</b> Two students (Yu Tan & Jia Ning) in my group received the National Scholarship for Graduate Students. </li>
                        
                        <li style="margin:10px"> <b>[Dec. 10, 2024]</b> 
                            "SQLFixAgent: Towards Semantic-Accurate Text-to-SQL Parsing" is accepted by AAAI 2025. </li>

                        <li style="margin:10px"> <b>[Dec. 1, 2024]</b> 
                            "TACL: A Trusted Action-enhanced Curriculum Learning Approach</a>" is accepted by Neurocomputing. </li>

                        <li style="margin:10px"> <b>[Jul. 16, 2024]</b> Two Papers ("Emotion-enriched Text-to-Motion Generation" & "Hawkeye") are accepted by ACM MM 2024. </li>
                        
                        <li style="margin:10px"> <b>[Feb. 20, 2024]</b> Three Papers ("ChatASU", "Weakly-supervised Phrase Grounding" & "TopicDiff") are accepted by COLING 2024. </li>
                        

                    </div>
                </div>
            </div>
        </div>
    </div>

    <div class="container" id="Recent-Research">
        <div class="row" id="Recent-Research">
            <div class="col-md-12">
                <h2>Recent Research</h2>
                <div class="page-header">
                    <h4>Foundation Model for Video Understanding</h4>
                </div>
                <img width="600" src="../img/hawkeye.svg" align="left" style="margin-left:0px;margin-right:20px"
                    class="model-img">
                <p>
                    The goal is to establish a unified framework for video anomaly detection, advancing precision in
                    identifying and localizing abnormal events across dynamic scenes while enabling interpretable
                    analysis
                    of complex visual patterns.
                </P>
                <p>
                    Starting from real-world applications in surveillance and social media analysis, we introduce
                    Hawkeye (<a target="_blank"
                        href="../works/Hawkeye Discovering and Grounding Implicit Anoma-lous Sentiment in Recon-videos via Scene-enhanced Video Large Language Model.pdf">Zhang
                        et al., ACM MM'24</a>)
                    , the first scene-enhanced video-language model designed for anomaly detection. Hawkeye
                    integrates multimodal context
                    (visual-textual-temporal cues) to recognize subtle anomalies and pinpoint their temporal
                    boundaries in untrimmed videos,
                    This work lays a critical foundation for event typing and spatiotemporal
                    localization in short video understanding.
                </p>
                <p>
                    Building on this, we investigate low-resource scenarios where annotated anomaly data is scarce.
                    Our Continuous
                    Attention Modeling method (<a target="_blank" href="../works/针对低资源场景下连续情感分析任务的持续注意力建模.pdf">Zhang et
                        al.,JOS'23</a>) enhances adaptability by capturing long-range
                    dependencies
                    in sparse anomaly signals. Further, we extend Hawkeye with self-supervised learning to uncover
                    latent patterns
                    across unlabeled videos, improving generalization to unseen anomaly types.
                    To scale solutions, we construct a benchmark suite combining large-scale anomaly annotations and
                    instruction-tuned datasets.
                    This addresses the challenge of diverse event types (e.g., accidents, unusual behaviors) and
                    supports downstream tasks
                    like explainable reasoning (<a target="_blank"
                        href="../works/ChatASU Evoking LLM's Reflexion to Truly Understand Aspect Sentiment in Dialogues.pdf">Liu
                        et al., COLING'2024</a>).
                </p>
                <!-- <p>
                    目标​​是构建统一的视频异常检测框架，在动态场景中提升异常事件识别与定位的精度，同时实现对复杂视觉模式的可解释分析。
                    ​从安防监控与社交媒体分析的实际需求出发​​，我们提出首个场景增强的视频-语言模型Hawkeye（ACM MM-2024）。该模型融合视觉、
                    文本与时序线索，能够识别短视频中细微的异常行为并精确定位其时空边界（如图2.1所示），为短时视频的事件类型划分与时空定位奠定重要基础。
                    ​​针对低资源场景​​（标注数据稀缺），我们提出连续注意力建模方法（软件学报，张等人），通过捕捉稀疏异常信号的长程依
                    赖关系提升模型适应性。进一步扩展Hawkeye至自监督学习框架，挖掘未标注视频的潜在模式以增强对未见异常类型的泛化能力
                    （赵等人，审稿中）。                
                    ​​为扩大解决方案规模​​，我们构建了融合大规模异常标注与指令微调数据集的基准测试套件，解决事件类型多样性（如事故、异常行为）
                    的挑战，并支持可解释推理等下游任务（刘等人，COLING-2024）。
                </p> -->
                <p>
                    <b>Papers: </b>
                    (<a target="_blank"
                        href="../works/Hawkeye Discovering and Grounding Implicit Anoma-lous Sentiment in Recon-videos via Scene-enhanced Video Large Language Model.pdf">Zhang
                        et al., ACM MM'24</a>)
                    (<a target="_blank" href="../works/针对低资源场景下连续情感分析任务的持续注意力建模.pdf">Zhang et al.,JOS'23</a>)
                    (<a target="_blank"
                        href="../works/ChatASU Evoking LLM's Reflexion to Truly Understand Aspect Sentiment in Dialogues.pdf">Liu
                        et al., COLING'2024</a>).
                </p>

                <div class="page-header">
                    <h4>Foundation Model for Multimodal Generation</h4>
                </div>
                <img width="600" src="../img/L3EM.svg" align="left"
                    style="margin-left:0px;margin-right:20px;margin-bottom:10px" class="model-img">
                <p>
                    The goal​​ is to establish an emotion-driven framework for multimodal large language model (LLM)
                    video generation
                    , achieving cross-modal semantic alignment and high-fidelity emotional action synthesis while
                    enhancing
                    controllability and realism in generated content.
                </p>
                <p>
                    Starting from the core demand of multimodal interaction, we propose LLM-Guided Emotion-Action
                    Synthesis
                    (<a target="_blank"
                        href="../works/Towards Emotion-enriched Text-to-Motion Generation via LLM-guided Limb-level Emotion Manipulating.pdf">Yu
                        et al., ACM MM'2024</a>),
                    the first method to jointly model text emotion semantics and human motion sequences for
                    emotionally rich video generation
                    This work enables precise conveyance of complex emotions (e.g., "excited waving") in synthetic
                    videos, providing critical support for emotion-aware short video editing.
                </p>
                <p>
                    To address cross-modal alignment challenges, we introduce two innovations:
                    1) A Trustworthy Reflection Mechanism
                    (<a target="_blank"
                        href="../works/ChatASU Evoking LLM's Reflexion to Truly Understand Aspect Sentiment in Dialogues.pdf">Liu
                        et al., COLING'24</a>)
                    that dynamically calibrates generation intent with emotional
                    labels, improving semantic coherence in action outputs;
                    2) Latent Diffusion Models
                    (<a target="_blank"
                        href="../works/How to Understand 'Support'? An Implicit-enhanced Causal Inference Approach for Weakly-supervised Phrase Grounding.pdf">Luo
                        et al., COLING'24</a>)
                    that jointly model emotion-themed distributions across text, audio,
                    and motion modalities in latent space, overcoming efficiency bottlenecks of pixel-level
                    generation.

                    To scale practical applications, we construct the first benchmark dataset for emotion-driven
                    action generation,
                    containing 100K+ emotion-annotated human motion sequences and multimodal instruction pairs.
                    Based on this, we propose
                    AutoEmoDirector, a framework enabling users to edit video emotion intensity and motion styles
                    via natural language instructions
                    (e.g., "transform a joyful dance into a sorrowful stroll"), significantly lowering the barrier
                    for video content creation.
                </p>

                <!-- <p>
                    目标​​是构建情感驱动的大模型视频生成框架，实现跨模态语义对齐与高保真情感动作合成，同时提升生成结果的可控性与真实性。
                    ​​从多模态交互的核心需求出发​​，我们提出LLM-Guided Emotion-Action Synthesis（ACM MM-2024），通过大语言模型引导肢体
                    动作的细粒度情感表达（如图2.2所示）。该方法首次将文本情感语义与人体运动序列联合建模，生成的视频能精准传递愤怒、喜
                    悦等复杂情感（如"激动地挥手"），为情感化短视频编辑提供关键技术支持。  
                    ​​针对跨模态对齐难题​​，我们提出两种创新方法：1）改进大语言模型的可信反思机制（Liu et al., COLING-2024），通过动态校
                    准生成意图与情感标签的一致性，提升动作生成的语义合理性；2）设计隐空间扩散模型（Luo et al., COLING-2024），在潜在
                    空间联合建模文本、音频与动作模态的情感主题分布，突破传统像素级生成的效率瓶颈。
                    ​​为推动规模化应用​​，我们构建了首个情感动作生成基准数据集，包含10万+条带情感标注的人体动作序列与多模态指令对。基于此，
                    我们提出AutoEmoDirector框架，支持用户通过自然语言指令实时编辑视频中的情感强度与动作风格（如"将欢快舞蹈转为悲伤漫步
                    "），显著降低视频创作门槛。
                </p> -->

                <p>
                    <b>Papers: </b>
                    (<a target="_blank"
                        href="../works/Towards Emotion-enriched Text-to-Motion Generation via LLM-guided Limb-level Emotion Manipulating.pdf">Yu
                        et al., ACM MM'2024</a>)
                    (<a target="_blank"
                        href="../works/How to Understand 'Support'? An Implicit-enhanced Causal Inference Approach for Weakly-supervised Phrase Grounding.pdf">Luo
                        et al., COLING'24</a>)
                    (<a target="_blank"
                        href="../works/ChatASU Evoking LLM's Reflexion to Truly Understand Aspect Sentiment in Dialogues.pdf">Liu
                        et al., COLING'24</a>)
                </p>
            </div>


        </div>
    </div>
    <div class="container" id="publication">
        <div class="row" id="publication">
            <div class="col-md-12">
                <h2>Publications</h2>
                <div class="sectionbody">
                    <div class="paragraph">
                        <p>2025</p>
                    </div>
                    <div class="olist arabic">
                        <ol class="arabic">
                            <li>
                                <p>
                                    Ma, J., <strong>Wang, J.</strong>, Luo, J., Yu, P., & Zhou, G. <em>Sherlock:
                                        Towards Multi-scene Video Abnormal Event Extraction and Localization via a
                                        Global-local Spatial-sensitive LLM</em>. The Web Conference
                                    (<strong>WWW</strong>), 2025. (CCF A)
                                </p>
                            </li>
                            <li>
                                <p>
                                    Luo, J., <strong>Wang, J.</strong>, Ma, J., Jin, Y., Li, S., & Zhou, G.
                                    <em>Omni-SILA: Towards Omni-scene Driven Visual Sentiment Identifying, Locating
                                        and Attributing in Videos</em>. The Web Conference (<strong>WWW</strong>),
                                    2025. (CCF A)
                                </p>
                            </li>
                            <li>
                                <p>
                                    Cen, J., Liu, J., Li, Z., & <strong>Wang, J.</strong> <em>SQLFixAgent: Towards
                                        Semantic-Accurate Text-to-SQL Parsing via Consistency-Enhanced Multi-Agent
                                        Collaboration</em>. The AAAI Conference on Artificial Intelligence
                                    (<strong>AAAI</strong>), 2025. (CCF A)
                                </p>
                            </li>
                            <li>
                                <p>
                                    Yu, T., <strong>Wang, J.</strong>, Luo, J., Wang, J., & Zhou, G. <em>TACL: A
                                        Trusted Action-enhanced Curriculum Learning Approach to Multimodal Affective
                                        Computing</em>. Neurocomputing, 2025, 620, 129195. (SCI Q2)
                                </p>
                            </li>
                        </ol>
                    </div>

                    <div class="paragraph">
                        <p>2024</p>
                    </div>
                    <div class="olist arabic">
                        <ol class="arabic">
                            <li>
                                <p>
                                    Zhao J, <strong>Wang, J.</strong>, Jin Y, et al. <em>Hawkeye: Discovering and
                                        Grounding Implicit Anomalous Sentiment in Recon-videos via Scene-enhanced
                                        Video Large Language Model</em>. Proceedings of ACM International Conference
                                    on Multimedia (<strong>MM</strong>), 2024, 592–601. (CCF A)
                                </p>
                            </li>
                            <li>
                                <p>
                                    Yu T, <strong>Wang, J.</strong>, Wang J, et al. <em>Towards Emotion-enriched
                                        Text-to-Motion Generation via LLM-guided Limb-level Emotion
                                        Manipulating</em>. Proceedings of ACM International Conference on Multimedia
                                    (<strong>MM</strong>), 2024, 612–621. (CCF A)
                                </p>
                            </li>
                            <li>
                                <p>
                                    Liu Y, <strong>Wang, J.</strong>, Luo J, et al. <em>ChatASU: Evoking LLM's
                                        Reflexion to Truly Understand Aspect Sentiment in Dialogues</em>.
                                    Proceedings of International Conference on Computational Linguistics
                                    (<strong>COLING</strong>), 2024, 3075–3085. (CCF B)
                                </p>
                            </li>
                            <li>
                                <p>
                                    Luo J, Zhao J, <strong>Wang, J.</strong>, et al. <em>How to Understand
                                        'Support'? An Implicit-enhanced Causal Inference Approach for
                                        Weakly-supervised Phrase Grounding</em>. International Conference on
                                    Computational Linguistics (<strong>COLING</strong>), 2024. (CCF B)
                                </p>
                            </li>
                            <li>
                                <p>
                                    Luo, J., <strong>Wang, J.</strong>, & Zhou, G. <em>TopicDiff: A Topic-enriched
                                        Diffusion Approach for Multimodal Conversational Emotion Detection</em>.
                                    International Conference on Computational Linguistics (<strong>COLING</strong>),
                                    2024. (CCF B)
                                </p>
                            </li>
                            <li>
                                <p>
                                    Luo, J., <strong>Wang, J.</strong>, & Zhou, G. <em>Topic-Enriched Variational
                                        Transformer for Conversational Emotion Detection</em>. CCF International
                                    Conference on Natural Language Processing and Chinese Computing, 2024, 3-15.
                                    (CCF C)
                                </p>
                            </li>
                        </ol>
                    </div>

                    <div class="paragraph">
                        <p>2023</p>
                    </div>
                    <div class="olist arabic">
                        <ol class="arabic">
                            <li>
                                <p>
                                    Luo JM, <strong>Wang JJ</strong>, Zhou GD. <em>Multi-modal Reliability-aware
                                        Affective Computing</em>. Ruan Jian Xue Bao/Journal of Software, 2023,
                                    36(2):537-553. (CCF A in Chinese Journal)
                                </p>
                            </li>
                            <li>
                                <p>
                                    Liu YD, <strong>Wang JJ</strong>, Luo JM, Zhou GD. <em>LLM-Grounded Conversation
                                        Aspect Sentiment Understanding via Muti-Agent Consistency Reflection</em>.
                                    Journal of Software, 2023. (CCF A in Chinese Journal)
                                </p>
                            </li>
                            <li>
                                <p>
                                    Zhang H, <strong>Wang JJ</strong>, Luo JM, Zhou GD. <em>Continual Attention
                                        Modeling for Sucessive Sentiment Analysis in Low resource Scenarios</em>.
                                    Ruan Jian Xue Bao/Journal of Software, 2023, 35(12):5470-5486. (CCF A in Chinese
                                    Journal)
                                </p>
                            </li>
                            <li>
                                <p>
                                    <strong>Wang, J.</strong>, Luo, J., & Zhou, G. <em>Fine-Grained Question-Answer
                                        Matching via Sentence-Aware Contrastive Self-supervised Transfer</em>. CCF
                                    International Conference on Natural Language Processing and Chinese Computing,
                                    2023, 616-628. (CCF C)
                                </p>
                            </li>
                        </ol>
                    </div>

                    <div class="paragraph">
                        <p>2022</p>
                    </div>
                    <div class="olist arabic">
                        <ol class="arabic">
                            <li>
                                <p>
                                    Gao X, <strong>Wang, J.</strong>, Li S, et al. <em>Cognition-driven multimodal
                                        personality classification</em>. Science China Information Sciences
                                    (<strong>SCIS</strong>), 2022, 65(10). (CCF A)
                                </p>
                            </li>
                        </ol>
                    </div>

                    <div class="paragraph">
                        <p>2021</p>
                    </div>
                    <div class="olist arabic">
                        <ol class="arabic">
                            <li>
                                <p>
                                    Gao, X., <strong>Wang, J.</strong>, Li, S., & Zhou, G. <em>Cognition-Driven
                                        Real-Time Personality Detection via Language-Guided Contrastive Visual
                                        Attention</em>. IEEE International Conference on Multimedia and Expo
                                    (<strong>ICME</strong>), 2021, 1-6. (CCF B)
                                </p>
                            </li>
                            <li>
                                <p>
                                    Liu YD, <strong>Wang JJ</strong>, Luo JM, Zhou GD. <em>LLM-Grounded Conversation
                                        Aspect Sentiment Understanding via Muti-Agent Consistency Reflection</em>.
                                    Journal of Software, 2021. (CCF A in Chinese Journal)
                                </p>
                            </li>
                            <li>
                                <p>
                                    Zhao JN, <strong>Wang JJ</strong>, Luo JM, Zhou GD. <em>Implicit-enhanced Causal
                                        Modeling for Phrasal Visual Grounding</em>. Ruan Jian Xue Bao/Journal of
                                    Software, 2021. (CCF A in Chinese Journal)
                                </p>
                            </li>
                        </ol>
                    </div>

                    <div class="paragraph">
                        <p>2020</p>
                    </div>
                    <div class="olist arabic">
                        <ol class="arabic">
                            <li>
                                <p>
                                    Chen X, Sun C, <strong>Wang, J.</strong>, et al. <em>Aspect Sentiment
                                        Classification with Document-level Sentiment Preference Modeling</em>.
                                    Proceedings of Annual Meeting of the Association for Computational Linguistics
                                    (<strong>ACL</strong>), 2020, 3667–3677. (CCF A)
                                </p>
                            </li>
                            <li>
                                <p>
                                    <strong>Wang, J.</strong>, Wang, J., Sun, C., Li, S., Liu, X., Si, L., ... &
                                    Zhou, G. <em>Sentiment classification in customer service dialogue with
                                        topic-aware multi-task learning</em>. Proceedings of the AAAI Conference on
                                    Artificial Intelligence (<strong>AAAI</strong>), 2020, 9177-9184. (CCF A)
                                </p>
                            </li>
                            <li>
                                <p>
                                    An, M., <strong>Wang, J.</strong>, Li, S., & Zhou, G. <em>Multimodal
                                        topic-enriched auxiliary learning for depression detection</em>. Proceedings
                                    of the International Conference on Computational Linguistics
                                    (<strong>COLING</strong>), 2020, 1078-1089. (CCF B)
                                </p>
                            </li>
                        </ol>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <div class="container">
        <div class="row" id="publication-before-2019">
            <div class="col-md-12">
                <div class="sectionbody">
                    <div class="paragraph">
                        <p>2019</p>
                    </div>
                    <div class="olist arabic">
                        <ol class="arabic">
                            <li>
                                <p>
                                    <strong>Jingjing Wang</strong>, Changlong Sun, Shoushan Li, Jiancheng Wang, Luo
                                    Si, Min Zhang, Xiaozhong Liu, Guodong Zhou. <em>Human-Like Decision Making:
                                        Document-level Aspect Sentiment Classification via Hierarchical
                                        Reinforcement Learning</em>. Proceedings of the Conference on Empirical
                                    Methods in Natural Language Processing (<strong>EMNLP</strong>), 2019,
                                    5585-5594. (CCF B)
                                </p>
                            </li>
                            <li>
                                <p>
                                    <strong>Jingjing Wang</strong>, Changlong Sun, Shoushan Li, Xiaozhong Liu, Min
                                    Zhang, Luo Si, Guodong Zhou. <em>Aspect Sentiment Classification Towards
                                        Question-Answering with Reinforced Bidirectional Attention Network</em>.
                                    Proceedings of Annual Meeting of the Association for Computational Linguistics
                                    (<strong>ACL</strong>), 2019. (CCF A)
                                </p>
                            </li>
                        </ol>
                    </div>

                    <div class="paragraph">
                        <p>2018</p>
                    </div>
                    <div class="olist arabic">
                        <ol class="arabic">
                            <li>
                                <p>
                                    <strong>Jingjing Wang</strong>, Jie Li, Shoushan Li, Yangyang Kang, Min Zhang,
                                    Luo Si, Guodong Zhou. <em>Aspect Sentiment Classification with both Word-level
                                        and Clause-level Attention Networks</em>. Proceedings of International Joint
                                    Conference on Artificial Intelligence (<strong>IJCAI</strong>), 2018, 4439-4445.
                                    (CCF A)
                                </p>
                            </li>
                            <li>
                                <p>
                                    <strong>Jingjing Wang</strong>, Shoushan Li, Mingqi Jiang, Hanqian Wu, Guodong
                                    Zhou. <em>Cross-media User Profiling with Joint Textual and Social User
                                        Embedding</em>. Proceedings of International Conference on Computational
                                    Linguistics (<strong>COLING</strong>), 2018, 246-251. (CCF B)
                                </p>
                            </li>
                            <li>
                                <p>
                                    Chenlin Shen, Changlong Sun, <strong>Jingjing Wang</strong>, Yangyang Kang,
                                    Shoushan Li, Xiaozhong Liu, Luo Si, Min Zhang, Guodong Zhou. <em>Sentiment
                                        Classification towards Question-Answering with Hierarchical Matching
                                        Network</em>. Proceedings of Conference on Empirical Methods in Natural
                                    Language Processing (<strong>EMNLP</strong>), 2018, 3654-3663. (CCF B)
                                </p>
                            </li>
                        </ol>
                    </div>

                    <div class="paragraph">
                        <p>2017</p>
                    </div>
                    <div class="olist arabic">
                        <ol class="arabic">
                            <li>
                                <p>
                                    <strong>Jingjing Wang</strong>, Shoushan Li, Guodong Zhou. <em>Joint Learning on
                                        Relevant User Attributes in Micro-blog</em>. Proceedings of International
                                    Joint Conference on Artificial Intelligence (<strong>IJCAI</strong>), 2017,
                                    4130-4136. (CCF A)
                                </p>
                            </li>
                        </ol>
                    </div>

                    <div class="paragraph">
                        <p>2015</p>
                    </div>
                    <div class="olist arabic">
                        <ol class="arabic">
                            <li>
                                <p>
                                    <strong>Jingjing Wang</strong>, Yunxia Xue, Shoushan Li, Guodong Zhou.
                                    <em>Leveraging Interactive Knowledge and Unlabeled Data in Gender Classification
                                        with Co-training</em>. Proceedings of International Conference on Database
                                    Systems for Advanced Applications (<strong>DASFAA</strong>), 2015, 246-251. (CCF
                                    B)
                                </p>
                            </li>
                            <li>
                                <p>
                                    Shoushan Li, <strong>Jingjing Wang</strong>, Guodong Zhou, Hanxiao Shi.
                                    <em>Interactive Gender Inference with Integer Linear Programming</em>.
                                    Proceedings of International Joint Conference on Artificial Intelligence
                                    (<strong>IJCAI</strong>), 2015, 2341-2347. (CCF A)
                                </p>
                            </li>
                            <li>
                                <p>
                                    Shoushan Li, Lei Huang, <strong>Jingjing Wang</strong>, Guodong Zhou.
                                    <em>Semi-Stacking for Semi-supervised Sentiment Classification</em>. Proceedings
                                    of Annual Meeting of the Association for Computational Linguistics
                                    (<strong>ACL</strong>), 2015, 27-31. (CCF A)
                                </p>
                            </li>
                            <li>
                                <p>
                                    Zhu zhu, <strong>Jingjing Wang</strong>, Shoushan Li, Guodong Zhou.
                                    <em>Interactive Gender Inference in Social Media</em>. Proceedings of
                                    International Conference on Database Systems for Advanced Applications
                                    (<strong>DASFAA</strong>), 2015, 252-258. (CCF B)
                                </p>
                            </li>
                        </ol>
                    </div>
                </div>
            </div>
        </div>
    </div>


    <div class="container" id="award">
        <div class="row" id="award">
            <div class="col-md-12">
                <h2>Awards & Honors</h2>
                <ul>

                    <li>
                        <p>Outstanding Expert and Supervisor of Microsoft (2022)</p>
                    </li>
                    <li>
                        <p>Outstanding Graduate PhD Student of Soochow University (2019)</p>
                    </li>
                    <li>
                        <p>Suzhou Industrial Park Scholarship (2018)</p>
                    </li>
                    <li>
                        <p>National Scholarship for Ph.D. (2017)</p>
                    </li>
                    <li>
                        <p>Ph.D. Scholarship of Soochow University (2017)</p>
                    </li>
                    <li>
                        <p>National Scholarship for Master (2016)</p>
                    </li>
                    <li>
                        <p>Outstanding Graduate Student of Soochow University (2016)</p>
                    </li>
                    <li>
                        <p>Suzhou Industrial Park Scholarship (2015) etc. </p>
                    </li>
                </ul>
            </div>
        </div>
    </div>

    <div class="container" id="academic-services">
        <div class="row" id="academic-services">
            <div class="col-md-12">
                <h3>Academic Services</h3>
                <ul>
                    <li>
                        <p><b>Technical Program Committee (Area Chair & PC)</b></p>
                    </li>
                    <li>
                        <p>ACL: Annual Meeting of the Association for Computational Linguistics, Area Chair</p>
                    </li>
                    <li>
                        <p>EMNLP: Conference on Empirical Methods in Natural Language Processing, Area Chair</p>
                    </li>
                    <li>
                        <p>AAAI: Association for the Advancement of Artificial Intelligence, PC</p>
                    </li>
                    <li>
                        <p>IJCAI: International Joint Conference on Artificial Intelligence, PC</p>
                    </li>
                    <li>
                        <p>etc.</p>
                    </li>

                    <li>
                        <p><b>Journal Reviewer</b></p>
                    </li>
                    <li>
                        <p>TASLP: IEEE/ACM Transactions on Audio, Speech, and Language Processing</p>
                    </li>
                    <li>
                        <p>TALLIP: ACM Transactions on Asian and Low-Resource Language Information Processing</p>
                    </li>
                    <li>
                        <p>SCIS: Science China Information Sciences</p>
                    </li>
                    <li>
                        <p>Science China</p>
                    </li>
                    <li>
                        <p>Acta Automatica Sinica</p>
                    </li>
                    <li>
                        <p>Journal of Chinese Information Processing</p>
                    </li>
                    <li>
                        <p>etc.</p>
                    </li>

                    <li>
                        <p><b>Academic Presentations and Exchanges</b></p>
                    </li>
                    <li>
                        <p>2016-2021: Academic reports and exchanges at top conferences including ACL, AAAI, IJCAI</p>
                    </li>
                    <li>
                        <p>2019: Academic report and exchange at Zhejiang Tailong Commercial Bank, Suzhou Industrial
                            Park Headquarters</p>
                    </li>
                    <li>
                        <p>2019: Invited talk at Ecovacs, Suzhou</p>
                    </li>
                    <li>
                        <p>2022: Academic report and exchange at Alibaba Ant Financial</p>
                    </li>
                    <li>
                        <p>2023: Academic report and exchange at the establishment of NLPAI-SCHOOL, Microsoft Asia
                            Engineering Institute, Suzhou</p>
                    </li>
                    <li>
                        <p>etc.</p>
                    </li>
                </ul>
            </div>
        </div>
    </div>

    <div class="container" id="research-grants">
        <div class="row" id="research-grants">
            <div class="col-md-12">
                <h2>Research Grants</h2>
                <div class="sectionbody">
                    <div class="paragraph">
                        <p><b>As Principle Investigator</b></p>
                    </div>
                    <div class="olist arabic">
                        <ol class="arabic">
                            <li>
                                <p>
                                    Key Technology Research on Attribute-level Sentiment Analysis for Conversational
                                    Texts (No. 62006166: 240K RMB: 2021.01–2023.12)<br>
                                    Supported by the National Natural Science Foundation of China (NSFC Young Scientist
                                    Fund Project)
                                </p>
                            </li>
                            <li>
                                <p>
                                    Research on Chinese Single-document Automatic Summarization Based on Discourse
                                    Structure Analysis (No. 61976146: 560K RMB: 2020.01–2023.12)<br>
                                    Supported by the National Natural Science Foundation of China (NSFC General Program)
                                </p>
                            </li>
                            <li>
                                <p>
                                    Resource Construction and Key Technology Research on Sentiment Information
                                    Extraction from Question-answer Texts (No. 2019M661930: 80K RMB:
                                    2020.01–2022.12)<br>
                                    Supported by China Postdoctoral Science Foundation (CPSF)
                                </p>
                            </li>
                        </ol>
                    </div>

                    <div class="paragraph">
                        <p><b>As Co-investigator</b></p>
                    </div>
                    <div class="olist arabic">
                        <ol class="arabic">
                            <li>
                                <p>
                                    Scene-based Knowledge Graph for Language Understanding and Generation (Sub-project
                                    No. 2020AAA0108604: 6,650K RMB: 2020.11–2023.10)<br>
                                    Supported by the National Key Research and Development Program
                                </p>
                            </li>
                        </ol>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="container" id="contact">
        <!-- Example row of columns -->

        <hr>

        <footer>
            <!-- <p>&copy; Jingjing Wang 2023</p> -->
        </footer>
    </div> <!-- /container -->


    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
    <script src="../dist/js/bootstrap.min.js"></script>
    <script src="../dist/js/particles.min.js"></script>
    <script>
        document.addEventListener('DOMContentLoaded', function () {
            particlesJS('particles', {
                "particles": {
                    "number": {
                        "value": 80,
                        "density": {
                            "enable": true,
                            "value_area": 800
                        }
                    },
                    "color": {
                        "value": "#cccccc"
                    },
                    "shape": {
                        "type": "circle",
                        "stroke": {
                            "width": 0,
                            "color": "#000000"
                        }
                    },
                    "opacity": {
                        "value": 0.5,
                        "random": false
                    },
                    "size": {
                        "value": 3,
                        "random": true
                    },
                    "line_linked": {
                        "enable": true,
                        "distance": 150,
                        "color": "#cccccc",
                        "opacity": 0.4,
                        "width": 1
                    },
                    "move": {
                        "enable": true,
                        "speed": 2,
                        "direction": "none",
                        "random": false,
                        "straight": false,
                        "out_mode": "out",
                        "bounce": false
                    }
                },
                "interactivity": {
                    "detect_on": "canvas",
                    "events": {
                        "onhover": {
                            "enable": true,
                            "mode": "grab"
                        },
                        "onclick": {
                            "enable": true,
                            "mode": "push"
                        },
                        "resize": true
                    },
                    "modes": {
                        "grab": {
                            "distance": 140,
                            "line_linked": {
                                "opacity": 1
                            }
                        },
                        "push": {
                            "particles_nb": 4
                        }
                    }
                },
                "retina_detect": true
            });
        });
    </script>
</body>

</html>
